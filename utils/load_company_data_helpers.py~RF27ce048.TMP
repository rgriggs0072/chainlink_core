# utils/load_company_data_helpers.py
"""
Load Company Data Helpers

Overview for future devs:
- Houses all formatting & upload helpers for:
    * SALES_REPORT
    * CUSTOMERS
    * PRODUCTS
    * SUPPLIER_COUNTY
- Contains both:
    * Legacy Excel-based formatters (openpyxl) kept for rollback
    * New DataFrame-based validation/template logic (starting with CUSTOMERS)
- All Snowflake writes use:
    * Tenant-aware connection via connect_to_tenant_snowflake
    * TRUNCATE + INSERT pattern wrapped in explicit transactions
    * Audit fields: TENANT_ID, CREATED_AT, UPDATED_AT, LAST_LOAD_DATE

Notes:
- This module assumes st.session_state["toml_info"] and ["tenant_id"] are set
  by the login / tenant bootstrap logic before uploads are called.
"""

from datetime import datetime
from io import BytesIO

import numpy as np
import pandas as pd
import streamlit as st
from openpyxl.styles import numbers

from sf_connector.service_connector import connect_to_tenant_snowflake
from utils.class_validation_helpers import (
    ColumnRule,
    ValidationResult,
    validate_dataframe,
)

# =============================================================================
# 🔧 LEGACY FORMATTERS (Excel / openpyxl)
# =============================================================================


def format_sales_report(workbook):
    """
    Legacy formatter for the 'SALES REPORT' worksheet.

    Behavior (old app path):
    - Keeps only the 'SALES REPORT' sheet
    - Removes header row 2 and extra columns
    - Strips hyphens from UPC column
    - Splits store name to remove '#XXXX' part
    - Cleans commas/apostrophes from key text columns
    - Replaces 'Is Null' with 0 in numeric column
    - Normalizes a numeric column (G) to plain number format

    This is kept for rollback/compat purposes while the new
    template/validator-based flow is rolled out.
    """
    try:
        for sheet_name in workbook.sheetnames:
            if sheet_name != "SALES REPORT":
                workbook.remove(workbook[sheet_name])

        ws = workbook["SALES REPORT"]

        # Remove second header row & extra columns
        ws.delete_rows(2)
        ws.delete_cols(8)

        # Clean UPC (strip hyphens)
        for cell in ws["F"]:
            if cell.value is not None:
                cell.value = str(cell.value).replace("-", "")

        # Insert STORE NAME column
        ws.insert_cols(2)
        ws.cell(row=1, column=2, value="STORE NAME")

        # Extract clean store name from 'Customer Name' (#XXXX stripped)
        for row in ws.iter_rows(min_row=2, min_col=3, max_col=3):
            for cell in row:
                raw = str(cell.value) if cell.value is not None else ""
                if "#" in raw:
                    store_name = raw.split("#")[0].replace("'", "")
                else:
                    store_name = raw.replace("'", "")
                ws.cell(row=cell.row, column=2).value = store_name

        # Drop original customer name column
        ws.delete_cols(3)

        # Clean text columns
        for col in ["B", "E", "C"]:
            for cell in ws[col]:
                if cell.value and isinstance(cell.value, str):
                    cell.value = (
                        cell.value.replace(",", " ")
                        .replace(" 's", "")
                        .replace("'", "")
                    )

        # Replace 'Is Null' with 0 in F
        for cell in ws["F"]:
            if cell.value is not None:
                cell.value = str(cell.value).replace("Is Null", "0")

        # Normalize numeric column G
        for cell in ws["G"][1:]:
            try:
                if isinstance(cell.value, str):
                    cell.value = float(cell.value.replace(",", ""))
                cell.number_format = numbers.FORMAT_NUMBER
            except Exception:
                # Best-effort; ignore individual conversion failures
                pass

        return workbook

    except Exception as e:
        st.error(f"Error formatting sales report: {str(e)}")
        return None


def format_customers_report(workbook):
    """
    Legacy formatter for the 'Customers' worksheet.

    Behavior (old app path):
    - Keeps only 'Customers' sheet
    - Removes auto-filter and extra sheets
    - Repositions and cleans store number & store name
    - Enforces numeric-only Store_Number
    - Writes a fixed header row matching legacy upload expectations

    This is kept for rollback/compat while the new validation flow
    (template-driven) is rolled out.
    """
    try:
        for sheet_name in workbook.sheetnames:
            if sheet_name != "Customers":
                workbook.remove(workbook[sheet_name])

        ws = workbook["Customers"]
        ws.auto_filter.ref = None
        ws.insert_cols(3)

        # Strip '#XXXX' from store-name column (col D)
        for row in ws.iter_rows(min_row=2, min_col=4, max_col=4):
            for cell in row:
                if "#" in str(cell.value):
                    ws.cell(row=cell.row, column=4).value = str(cell.value).split("#")[0]

        # Move store_number into col C
        for row in ws.iter_rows(min_row=2, min_col=5, max_col=5):
            ws.cell(row=row[0].row, column=3).value = row[0].value

        ws.delete_cols(5)

        # Legacy header set
        headers = [
            "Customer_id",
            "Chain_Name",
            "Store_Number",
            "Store_Name",
            "Address",
            "City",
            "County",
            "Salesperson",
            "Account_Status",
        ]
        for col_idx, header in enumerate(headers, start=1):
            ws.cell(row=1, column=col_idx, value=header)

        # Clean stray quotes
        for col in ["B", "E"]:
            for cell in ws[col]:
                if cell.value and isinstance(cell.value, str):
                    cell.value = cell.value.replace("'", "")

        # Validate that Store_Number is numeric
        invalid_rows = []
        for cell in ws["C"][1:]:
            if cell.value and not str(cell.value).isdigit():
                invalid_rows.append((cell.row, cell.value))

        if invalid_rows:
            st.error("Non-numeric Store Numbers found:")
            for row_num, val in invalid_rows:
                st.warning(f"Row {row_num}: '{val}'")
            st.stop()

        return workbook

    except Exception as e:
        st.error(f"Error formatting Customers sheet: {str(e)}")
        return None


# =============================================================================
# 🧪 NEW CUSTOMERS TEMPLATE + VALIDATION LAYER
# =============================================================================


def _normalize_str_series(series: pd.Series) -> pd.Series:
    """
    Normalize string columns:
    - Cast to str
    - Strip whitespace
    - Convert obvious 'nan'/'None' to real nulls
    """
    s = series.astype(str).str.strip()
    s = s.replace({"nan": None, "None": None, "": None})
    return s


# Schema must match CUSTOMERS table (minus server-side audit cols).
CUSTOMERS_SCHEMA = [
    ColumnRule("CUSTOMER_ID", required=True, dtype="int"),
    ColumnRule("CHAIN_NAME", required=True, dtype="str"),
    ColumnRule("STORE_NUMBER", required=True, dtype="int"),
    ColumnRule("STORE_NAME", required=True, dtype="str"),
    ColumnRule("ADDRESS", required=True, dtype="str"),
    ColumnRule("CITY", required=True, dtype="str"),
    ColumnRule("COUNTY", required=True, dtype="str"),
    ColumnRule("SALESPERSON", required=True, dtype="str", allow_blank=False),
    ColumnRule("ACCOUNT_STATUS", required=True, dtype="str"),
    # TENANT_ID, CREATED_AT, UPDATED_AT, LAST_LOAD_DATE handled server-side.
]


def _normalize_store_name(name: str | None) -> str:
    """
    Normalize store names for comparison:
    - Treat None / NaN as empty string
    - Uppercase
    - Strip whitespace
    - Strip everything after a '#' (e.g. 'SMART & FINAL #829' -> 'SMART & FINAL')
    """
    if name is None:
        return ""
    s = str(name).upper().strip()
    if "#" in s:
        s = s.split("#", 1)[0].strip()
    return s



def _clean_upc(value):
    """
    Normalize UPC-like values by stripping all non-digit characters.

    Example:
        '8-5000079211-1' -> '850000792111'
        'ABC123XYZ' -> '123'
        '' / None / 'nan' -> None
    """
    if value is None:
        return None

    s = str(value).strip()
    if not s or s.lower() in ("nan", "none"):
        return None

    digits_only = "".join(ch for ch in s if ch.isdigit())
    return digits_only or None


def _validate_sales_upc(series: pd.Series) -> list[str]:
    """
    Per-row UPC validator for SALES_REPORT uploads.

    Rules:
      - After cleaning (digits only), UPC must not be empty.
      - Length must be 11 or 12 digits (standard UPC formats).
      - We report row numbers relative to a 1-based Excel-style sheet:
        assume header is row 1, so DataFrame index 0 => row 2, etc.
    """
    errors: list[str] = []

    for idx, raw in series.items():
        row_num = idx + 2  # header is row 1 in the typical Excel template
        cleaned = _clean_upc(raw)

        # Completely missing / no digits at all
        if cleaned is None:
            errors.append(
                f"Row {row_num}: UPC is missing or contains no usable digits (value={repr(raw)})."
            )
            continue

        # Must be all digits (paranoid check; _clean_upc already strips non-digits)
        if not cleaned.isdigit():
            errors.append(
                f"Row {row_num}: UPC '{raw}' cleaned to '{cleaned}', which still contains non-digit characters."
            )
            continue

        # Length check: typical UPC-A / EAN-like values are 11 or 12 digits
        if len(cleaned) not in (11, 12):
            errors.append(
                f"Row {row_num}: UPC '{raw}' cleaned to '{cleaned}' has length {len(cleaned)}; "
                f"expected 11 or 12 digits."
            )

    return errors


def _validate_purchased_flag(series: pd.Series) -> list[str]:
    """
    Validator for PURCHASED_YES_NO:
      - Must be 0 or 1 for every row.
    """
    errors: list[str] = []

    for idx, raw in series.items():
        row_num = idx + 2  # again, header = row 1
        if pd.isna(raw):
            errors.append(
                f"Row {row_num}: PURCHASED_YES_NO is blank; expected 0 or 1."
            )
            continue

        try:
            val = int(raw)
        except Exception:
            errors.append(
                f"Row {row_num}: PURCHASED_YES_NO '{raw}' is not an integer; expected 0 or 1."
            )
            continue

        if val not in (0, 1):
            errors.append(
                f"Row {row_num}: PURCHASED_YES_NO '{raw}' is not valid; expected 0 or 1."
            )

    return errors




def generate_customers_template() -> pd.DataFrame:
    """
    Create an empty Customers template DataFrame matching CUSTOMERS_SCHEMA.

    This is what users should paste into from Encompass (or other sources).
    """
    cols = [rule.name for rule in CUSTOMERS_SCHEMA]
    return pd.DataFrame(columns=cols)


def format_customers_upload(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Light normalization for Customers uploads using the OFFICIAL template.

    Rules:
    - Columns must match template names (case-insensitive, spaces/underscores ignored).
    - Normalize column names into canonical uppercase-underscore form.
    - Normalize key text columns (strip, handle nan/None).
    """
    df = raw_df.copy()

    def _norm(name: str) -> str:
        return name.strip().lower().replace(" ", "").replace("_", "")

    incoming_cols = list(df.columns)
    norm_map = {_norm(c): c for c in incoming_cols}

    target_cols = [rule.name for rule in CUSTOMERS_SCHEMA]

    rename = {}
    for target in target_cols:
        norm_target = _norm(target)
        if norm_target in norm_map:
            # Map whatever the user had (Customer_id, customer id, etc.) to our canonical name
            rename[norm_map[norm_target]] = target

    df = df.rename(columns=rename)

    # After renaming, clean up headers
    df.columns = [c.strip() for c in df.columns]

    # Normalize strings on key text columns
    for col in [
        "CHAIN_NAME",
        "STORE_NAME",
        "ADDRESS",
        "CITY",
        "COUNTY",
        "SALESPERSON",
        "ACCOUNT_STATUS",
    ]:
        if col in df.columns:
            df[col] = _normalize_str_series(df[col])

    return df


def validate_customers_upload(df: pd.DataFrame) -> ValidationResult:
    """
    Apply schema-based validation to Customers upload:
    - Required columns
    - Types (int/str)
    - Non-blank required fields
    - Drops & warns on extra columns
    """
    return validate_dataframe(df, CUSTOMERS_SCHEMA)


def validate_customers_against_existing_chains(
    df: pd.DataFrame,
) -> tuple[list[str], list[str]]:
    """
    Cross-validate CHAIN_NAME values in the upload against existing CUSTOMERS
    for this tenant, to catch issues like 'FOODMAXX' vs 'FOOD MAXX'.

    Returns:
        (errors, warnings)
    """
    conn, cursor, tenant_id = _get_conn_and_cursor()
    if not conn or not tenant_id:
        # If we don't have context, skip this check quietly.
        return [], []

    try:
        cursor.execute(
            "SELECT DISTINCT CHAIN_NAME FROM CUSTOMERS WHERE TENANT_ID = %s",
            (tenant_id,),
        )
        rows = cursor.fetchall()
    except Exception:
        # If query fails for any reason, don't block the upload on this cross-check
        return [], []
    finally:
        try:
            cursor.close()
        except Exception:
            pass
        try:
            conn.close()
        except Exception:
            pass

    known_raw = [r[0] for r in rows if r[0] is not None]
    if not known_raw:
        # First load for this tenant: nothing to compare yet
        return [], []

    def norm_name(s: str) -> str:
        # Uppercase and remove spaces for normalization
        return "".join(str(s).upper().split())

    # Map from normalized -> original name
    known_norm_map = {norm_name(x): x for x in known_raw}

    upload_chains = df["CHAIN_NAME"].dropna().astype(str)
    upload_unique = sorted(upload_chains.unique())

    errors: list[str] = []
    warnings: list[str] = []

    for val in upload_unique:
        n = norm_name(val)
        if n not in known_norm_map:
            errors.append(
                f"Chain '{val}' does not match any existing CHAIN_NAME for this tenant."
            )
        else:
            existing = known_norm_map[n]
            if existing != val:
                warnings.append(
                    f"Chain '{val}' differs from existing '{existing}'. "
                    f"Consider standardizing to '{existing}' for consistency."
                )

    return errors, warnings


# =============================================================================
# 🧪 SALES REPORT TEMPLATE + VALIDATION LAYER
# =============================================================================

# Schema must match SALES_REPORT table (minus audit/audit-like columns).
SALES_SCHEMA = [
    ColumnRule("STORE_NUMBER",   required=True, dtype="int"),
    ColumnRule("STORE_NAME",     required=True, dtype="str"),
    ColumnRule("ADDRESS",        required=True, dtype="str"),
    ColumnRule("SALESPERSON",    required=True, dtype="str"),
    ColumnRule("PRODUCT_NAME",   required=True, dtype="str"),
    ColumnRule(
        "UPC",
        required=True,
        dtype="str",
        allow_blank=False,
        validators=[_validate_sales_upc],   # ⬅ hook in our custom UPC validator
    ),
    ColumnRule(
        "PURCHASED_YES_NO",
        required=True,
        dtype="int",
        allow_blank=False,
        validators=[_validate_purchased_flag],
    ),
    # TENANT_ID, CHAIN_NAME, CREATED_AT, LAST_LOAD_DATE are set server-side.
]


def generate_sales_template() -> pd.DataFrame:
    """
    Create an empty Sales Report template DataFrame matching SALES_SCHEMA.

    This is what users should paste into instead of raw Encompass exports.
    """
    cols = [rule.name for rule in SALES_SCHEMA]
    return pd.DataFrame(columns=cols)


def format_sales_upload(raw_df: pd.DataFrame) -> pd.DataFrame:
    """
    Normalize a Sales Report upload that uses the OFFICIAL template.

    Rules:
    - Columns can be case-insensitive and ignore spaces/underscores.
    - We rename headers into canonical names in SALES_SCHEMA.
    - We normalize:
        * STORE_NAME: strip '#XXXX' suffix, remove quotes, uppercase.
        * UPC: digits only via _clean_upc().
        * Text fields: trimmed, normalized via _normalize_str_series.
    """
    df = raw_df.copy()

    # Header normalization similar to Customers
    def _norm(name: str) -> str:
        return name.strip().lower().replace(" ", "").replace("_", "")

    incoming_cols = list(df.columns)
    norm_map = {_norm(c): c for c in incoming_cols}
    target_cols = [rule.name for rule in SALES_SCHEMA]

    rename = {}
    for target in target_cols:
        norm_target = _norm(target)
        if norm_target in norm_map:
            rename[norm_map[norm_target]] = target

    # Rename to canonical names where possible
    df = df.rename(columns=rename)

    # Clean headers
    df.columns = [c.strip() for c in df.columns]

    # Clean STORE_NAME if present (strip chain # suffix, upper, remove quotes)
    if "STORE_NAME" in df.columns:
        df["STORE_NAME"] = (
            df["STORE_NAME"]
            .astype(str)
            .str.upper()
            .str.replace("'", "", regex=False)
            .str.split("#")
            .str[0]
            .str.strip()
        )

    # Clean UPC if present (digits only)
    if "UPC" in df.columns:
        df["UPC"] = df["UPC"].apply(_clean_upc)

    # Normalize basic text fields
    for col in ["ADDRESS", "SALESPERSON", "PRODUCT_NAME"]:
        if col in df.columns:
            df[col] = _normalize_str_series(df[col])

    return df


def validate_sales_upload(df: pd.DataFrame) -> ValidationResult:
    """
    Apply schema-based validation to Sales Report upload:
    - Required columns
    - Types (int/str)
    - Non-blank required fields
    - Drops & warns on extra columns
    """
    return validate_dataframe(df, SALES_SCHEMA)


def _build_missing_store_number_error(df: pd.DataFrame, missing_values: set[int]) -> str:
    """
    Build a detailed message showing which Excel rows contain STORE_NUMBERs
    that do NOT exist in CUSTOMERS for this tenant.
    """
    lines = [
        "Some STORE_NUMBER values in your upload do not exist in the CUSTOMERS table for this tenant:",
        "Problem rows (Excel row numbers):",
        "",
    ]

    for idx, row in df.iterrows():
        try:
            sn = int(row["STORE_NUMBER"])
        except Exception:
            continue

        if sn in missing_values:
            excel_row = idx + 2  # header is row 1
            store_name = str(row.get("STORE_NAME", "")).strip()
            lines.append(
                f"• Row {excel_row}: STORE_NUMBER={sn}, STORE_NAME='{store_name}'"
            )

    lines.append("")
    lines.append(
        "To fix: correct these STORE_NUMBER values in your file or add these stores "
        "to the Customers table first, then re-upload."
    )

    return "\n".join(lines)



def validate_sales_against_customers(df: pd.DataFrame) -> tuple[list[str], list[str]]:
    """
    Cross-validate STORE_NUMBER values in the Sales upload against existing CUSTOMERS.
    Returns (errors, warnings) where each error is a separate list entry.
    """

    conn, cursor, tenant_id = _get_conn_and_cursor()
    if not conn or not tenant_id:
        return [], []

    try:
        cursor.execute(
            """
            SELECT DISTINCT STORE_NUMBER, STORE_NAME
            FROM CUSTOMERS
            WHERE TENANT_ID = %s
              AND STORE_NUMBER IS NOT NULL;
            """,
            (tenant_id,),
        )
        rows = cursor.fetchall()
    except Exception:
        return [], []
    finally:
        try: cursor.close()
        except: pass
        try: conn.close()
        except: pass

    # Map for valid stores
    valid_store_numbers = {int(r[0]) for r in rows if r[0] is not None}

    # Map to help error messages
    df["_excel_row"] = df.index + 2  # Excel rows start at 2
    df["_store_num_int"] = pd.to_numeric(df["STORE_NUMBER"], errors="coerce")

    errors = []
    warnings = []

    # 1) Detect STORE_NUMBER rows that cannot be parsed to an integer
    unparseable = df[df["_store_num_int"].isna()]
    for _, row in unparseable.iterrows():
        excel_row = row["_excel_row"]
        raw_value = row["STORE_NUMBER"]
        errors.append(
            f"Row {excel_row}: STORE_NUMBER='{raw_value}' is not a valid integer."
        )

    # 2) Detect STORE_NUMBER values not found in CUSTOMERS
    checkable = df[df["_store_num_int"].notna()]
    for _, row in checkable.iterrows():
        sn = int(row["_store_num_int"])
        if sn not in valid_store_numbers:
            excel_row = row["_excel_row"]
            store_name = row.get("STORE_NAME", "")
            errors.append(
                f"Row {excel_row}: STORE_NUMBER={sn}, STORE_NAME='{store_name}' does not exist in CUSTOMERS."
            )

    # Cleanup
    df.drop(columns=["_excel_row", "_store_num_int"], inplace=True, errors="ignore")

    return errors, warnings








# =============================================================================
# 📊 SUPPLIER BY COUNTY FORMATTER
# =============================================================================


def format_supplier_by_county(file_content) -> pd.DataFrame:
    """
    Formats the uploaded Supplier by County pivot table Excel file into a normalized DataFrame
    ready for upload to the Snowflake SUPPLIER_COUNTY table.

    - Expects a sheet named 'Report'
    - Drops 'TOTAL' column if present
    - Renames 'Supplier / County' to 'Supplier'
    - Unpivots county columns into a single 'County' column
    - Converts values (1 → 'Yes', NaN → 'No')
    """
    try:
        xls = pd.ExcelFile(file_content)

        if "Report" not in xls.sheet_names:
            st.error("❌ Sheet named 'Report' not found in the Excel file.")
            st.info(
                "Please rename the sheet you want formatted to 'Report' and try again."
            )
            return None

        df = xls.parse("Report")

        if "Supplier / County" not in df.columns:
            st.error("❌ Column 'Supplier / County' not found in 'Report' sheet.")
            return None

        if "TOTAL" in df.columns:
            df = df.drop(columns=["TOTAL"])

        df.rename(columns={"Supplier / County": "Supplier"}, inplace=True)

        df_melted = pd.melt(
            df, id_vars=["Supplier"], var_name="County", value_name="Status"
        )

        df_melted["Status"] = df_melted["Status"].apply(
            lambda x: "Yes" if x == 1 else "No" if pd.isna(x) else str(x)
        )

        st.success("✅ Supplier by County formatting complete.")
        return df_melted

    except Exception as e:
        st.error(f"❌ Failed to format Supplier by County report: {str(e)}")
        return None


# =============================================================================
# 📦 PRODUCTS FORMATTER
# =============================================================================


def format_product_workbook(workbook):
    """
    Cleans and normalizes the 'Products' worksheet for upload.

    - Moves, cleans, and renames columns
    - Removes commas, apostrophes, hyphens
    - Sets PRODUCT_MANAGER column and removes unused columns
    """
    try:
        ws = workbook["Products"]

        # Move col G into col B
        col_g_data = [cell.value for cell in ws["G"]]
        ws.insert_cols(2)
        for cell, value in zip(ws["B"], col_g_data):
            cell.value = value
        for cell in ws["G"]:
            cell.value = None

        # Move col E into col D and drop original
        col_e_data = [cell.value for cell in ws["E"]]
        for cell, value in zip(ws["D"], col_e_data):
            cell.value = value
        ws.delete_cols(5)

        # Strip hyphens from carrier UPC column (E)
        for cell in ws["E"]:
            if isinstance(cell.value, str):
                cell.value = cell.value.replace("-", "")

        # Clean all string cells
        for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
            for cell in row:
                if isinstance(cell.value, str):
                    cell.value = cell.value.replace(",", "").replace("'", "")

        # PRODUCT_MANAGER column
        ws["F1"].value = "PRODUCT_MANAGER"
        for cell in ws["F"][1:]:
            cell.value = None

        # Drop any columns beyond F
        if ws.max_column >= 7:
            ws.delete_cols(7)

        # Fill missing UPC with placeholder numeric
        for cell in ws["E"][1:]:
            if cell.value is None:
                cell.value = 999999999999

        return workbook

    except Exception as e:
        st.error(f"❌ Error formatting product data: {str(e)}")
        return None


# =============================================================================
# 📤 DOWNLOAD HELPER
# =============================================================================


def download_workbook(workbook, filename: str) -> None:
    """
    Stream an openpyxl workbook to the user as an .xlsx download.
    """
    stream = BytesIO()
    workbook.save(stream)
    stream.seek(0)
    st.download_button(
        label="Download formatted file",
        data=stream.read(),
        file_name=filename,
        mime="application/vnd.ms-excel",
    )


# =============================================================================
# 🔐 SNOWFLAKE CONNECTION + TRANSACTION HELPERS
# =============================================================================


def _get_conn_and_cursor():
    """
    Open a tenant-scoped Snowflake connection and return (conn, cursor, tenant_id).

    Requires:
        st.session_state["toml_info"]
        st.session_state["tenant_id"]
    """
    toml_info = st.session_state.get("toml_info")
    tenant_id = st.session_state.get("tenant_id")
    if not toml_info or not tenant_id:
        st.error("❌ Missing tenant configuration.")
        return None, None, None

    conn = connect_to_tenant_snowflake(toml_info)
    if not conn:
        st.error("❌ Failed to connect to Snowflake.")
        return None, None, None

    return conn, conn.cursor(), tenant_id


def _finalize_transaction(cursor, conn, success_msg: str) -> None:
    """
    Commit transaction, close cursor + connection, and show success message.
    """
    conn.commit()
    cursor.close()
    conn.close()
    st.success(success_msg)


def _rollback_transaction(conn, cursor) -> None:
    """
    Rollback transaction and cleanly close cursor + connection.
    """
    if conn:
        conn.rollback()
    if cursor:
        cursor.close()
    if conn:
        conn.close()


def _build_audit_fields() -> tuple[str, str]:
    """
    Build standard audit fields:
        - now_ts: current timestamp as string
        - today_date: current date as YYYY-MM-DD
    """
    now_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    today_date = datetime.today().strftime("%Y-%m-%d")
    return now_ts, today_date


# =============================================================================
# 🔍 SALES ENRICHMENT HELPERS
# =============================================================================


def _fetch_customer_chain_lookup(conn, tenant_id: str) -> dict:
    """
    Pulls STORE_NUMBER, STORE_NAME, CHAIN_NAME from CUSTOMERS for this tenant.

    Normalizes STORE_NAME for consistency (upper, strip, '#xxx' removed).
    Returns:
        dict[int, str]: {store_number: chain_name}
    """
    lookup: dict[int, str] = {}

    try:
        with conn.cursor() as cur:
            cur.execute(
                """
                SELECT STORE_NUMBER, STORE_NAME, CHAIN_NAME
                FROM CUSTOMERS
                WHERE TENANT_ID = %s;
                """,
                (tenant_id,),
            )
            rows = cur.fetchall()

        for store_number, store_name, chain_name in rows:
            if store_number is None:
                continue

            norm_name = str(store_name or "").upper().strip()
            norm_name = norm_name.split("#")[0].strip()

            lookup[int(store_number)] = chain_name

        return lookup

    except Exception as e:
        st.warning(f"⚠️ Could not enrich CHAIN_NAME from CUSTOMERS: {e}")
        return {}



# =============================================================================
# 📥 SALES_REPORT UPLOAD
# =============================================================================


def write_salesreport_to_snowflake(df: pd.DataFrame):
    """
    Upload cleaned sales report to Snowflake.

    Enhancements:
    - Cleans UPC (digits only)
    - Cleans STORE_NAME ('SMART & FINAL #829' -> 'SMART & FINAL')
    - Enriches CHAIN_NAME from CUSTOMERS table per tenant (if column exists)
    - Uses TRUNCATE + INSERT wrapped in a transaction
    - Adds audit fields (TENANT_ID, CREATED_AT, UPDATED_AT, LAST_LOAD_DATE)

    NOTE:
    - This assumes SALES_REPORT has a CHAIN_NAME column. If it does not yet,
      either add it or remove CHAIN_NAME from the INSERT + records tuple.
    """
    df = df.copy()

    # Clean UPC
    if "UPC" in df.columns:
        df["UPC"] = df["UPC"].apply(_clean_upc)

    # Clean store name (strip '#xxxx', upper, strip quotes)
    if "STORE_NAME" in df.columns:
        df["STORE_NAME"] = (
            df["STORE_NAME"]
            .astype(str)
            .str.upper()
            .str.replace("'", "", regex=False)
            .str.split("#")
            .str[0]
            .str.strip()
        )

    conn, cursor, tenant_id = _get_conn_and_cursor()
    if not conn:
        return

    now_ts, today_date = _build_audit_fields()

    # Build CHAIN_NAME enrichment map
    chain_lookup = _fetch_customer_chain_lookup(conn, tenant_id)

    # Add CHAIN_NAME column based on STORE_NUMBER
    df["CHAIN_NAME"] = df["STORE_NUMBER"].apply(
        lambda sn: chain_lookup.get(int(sn)) if pd.notna(sn) else None
    )

    try:
        cursor.execute("BEGIN;")
        cursor.execute("TRUNCATE TABLE SALES_REPORT;")

        records = [
            (
                row.STORE_NUMBER,
                row.STORE_NAME,
                row.ADDRESS,
                row.SALESPERSON,
                row.PRODUCT_NAME,
                row.UPC,
                row.PURCHASED_YES_NO,
                tenant_id,
                now_ts,              # CREATED_AT
                None,                # SALE_DATE (NULL)
                row.CHAIN_NAME,
                today_date,          # LAST_LOAD_DATE
            )
    for row in df.itertuples(index=False)
]


        insert_sql = """
            INSERT INTO SALES_REPORT (
                STORE_NUMBER,
                STORE_NAME,
                ADDRESS,
                SALESPERSON,
                PRODUCT_NAME,
                UPC,
                PURCHASED_YES_NO,
                TENANT_ID,
                CREATED_AT,
                SALE_DATE,
                CHAIN_NAME,
                LAST_LOAD_DATE
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);


        """
        cursor.executemany(insert_sql, records)
        _finalize_transaction(cursor, conn, "✅ Sales report uploaded.")

    except Exception as e:
        _rollback_transaction(conn, cursor)
        st.error(f"❌ Sales report upload failed: {str(e)}")


# =============================================================================
# 📥 CUSTOMERS UPLOAD
# =============================================================================


def write_customers_to_snowflake(df: pd.DataFrame):
    """
    Upload validated customer data to Snowflake with full audit fields.

    - Uppercases all string-like values
    - Uses TRUNCATE + INSERT in a transaction
    """
    df = df.copy()
    df.fillna("NULL", inplace=True)
    df = df.applymap(lambda x: str(x).strip().upper() if pd.notna(x) else x)

    conn, cursor, tenant_id = _get_conn_and_cursor()
    if not conn:
        return

    now_ts, today_date = _build_audit_fields()

    try:
        cursor.execute("BEGIN;")
        cursor.execute("TRUNCATE TABLE CUSTOMERS;")

        records = [
            (
                row.CUSTOMER_ID,
                row.CHAIN_NAME,
                row.STORE_NUMBER,
                row.STORE_NAME,
                row.ADDRESS,
                row.CITY,
                row.COUNTY,
                row.SALESPERSON,
                row.ACCOUNT_STATUS,
                tenant_id,
                now_ts,
                now_ts,
                today_date,
            )
            for row in df.itertuples(index=False)
        ]

        insert_sql = """
            INSERT INTO CUSTOMERS (
                CUSTOMER_ID,
                CHAIN_NAME,
                STORE_NUMBER,
                STORE_NAME,
                ADDRESS,
                CITY,
                COUNTY,
                SALESPERSON,
                ACCOUNT_STATUS,
                TENANT_ID,
                CREATED_AT,
                UPDATED_AT,
                LAST_LOAD_DATE
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        cursor.executemany(insert_sql, records)
        _finalize_transaction(cursor, conn, "✅ Customers uploaded.")

    except Exception as e:
        _rollback_transaction(conn, cursor)
        st.error(f"❌ Customer upload failed: {str(e)}")


# =============================================================================
# 📥 PRODUCTS UPLOAD
# =============================================================================


def write_products_to_snowflake(df: pd.DataFrame):
    """
    Uploads cleaned products data to Snowflake with proper type handling and auditing.
    """
    df = df.copy()
    df.replace("NAN", np.nan, inplace=True)
    df.replace({np.nan: None}, inplace=True)
    df["CARRIER_UPC"] = df["CARRIER_UPC"].astype(str).str.strip()

    conn, cursor, tenant_id = _get_conn_and_cursor()
    if not conn:
        return

    now_ts, today_date = _build_audit_fields()

    try:
        cursor.execute("BEGIN;")
        cursor.execute("TRUNCATE TABLE PRODUCTS;")

        records = [
            (
                row.PRODUCT_ID,
                row.SUPPLIER,
                row.PRODUCT_NAME,
                row.PACKAGE,
                row.CARRIER_UPC,
                row.PRODUCT_MANAGER,
                tenant_id,
                now_ts,
                now_ts,
                today_date,
            )
            for row in df.itertuples(index=False)
        ]

        insert_sql = """
            INSERT INTO PRODUCTS (
                PRODUCT_ID,
                SUPPLIER,
                PRODUCT_NAME,
                PACKAGE,
                CARRIER_UPC,
                PRODUCT_MANAGER,
                TENANT_ID,
                CREATED_AT,
                UPDATED_AT,
                LAST_LOAD_DATE
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        cursor.executemany(insert_sql, records)
        _finalize_transaction(cursor, conn, "✅ Products uploaded.")

    except Exception as e:
        _rollback_transaction(conn, cursor)
        st.error(f"❌ Product upload failed: {str(e)}")


# =============================================================================
# 📥 SUPPLIER_COUNTY UPLOAD
# =============================================================================


def write_supplier_by_county_to_snowflake(df: pd.DataFrame):
    """
    Uploads formatted Supplier by County DataFrame to SUPPLIER_COUNTY in Snowflake.

    - Tenant-aware connection
    - TRUNCATE + INSERT in a transaction
    - Adds audit fields: TENANT_ID, CREATED_AT, UPDATED_AT, LAST_LOAD_DATE
    """
    toml_info = st.session_state.get("toml_info")
    tenant_id = st.session_state.get("tenant_id")

    if not toml_info or not tenant_id:
        st.error("❌ Tenant configuration is missing.")
        return

    try:
        conn = connect_to_tenant_snowflake(toml_info)
        if not conn:
            st.error("❌ Failed to connect to Snowflake.")
            return

        cursor = conn.cursor()
        now_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        today_date = datetime.today().strftime("%Y-%m-%d")

        cursor.execute("BEGIN;")
        cursor.execute("TRUNCATE TABLE SUPPLIER_COUNTY;")

        records = [
            (
                row.Supplier,
                row.County,
                row.Status,
                tenant_id,
                now_ts,
                now_ts,
                today_date,
            )
            for row in df.itertuples(index=False)
        ]

        insert_sql = """
            INSERT INTO SUPPLIER_COUNTY (
                SUPPLIER,
                COUNTY,
                STATUS,
                TENANT_ID,
                CREATED_AT,
                UPDATED_AT,
                LAST_LOAD_DATE
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s);
        """

        cursor.executemany(insert_sql, records)
        conn.commit()
        st.success("✅ Supplier by County data uploaded successfully to Snowflake.")

    except Exception as e:
        if "conn" in locals():
            conn.rollback()
        st.error(f"❌ Supplier by County upload failed: {str(e)}")

    finally:
        if "cursor" in locals():
            cursor.close()
        if "conn" in locals():
            conn.close()
